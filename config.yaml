# Configuration for Infinitely Scalable Recursive Model (ISRM)
# ============================================================
# The world's first model where quality scales with inference loops!

model:
  # Architecture
  dim: 384                    # Increased for better learning
  n_layers: 4                 # Deeper network
  n_heads: 6                  # More heads
  n_kv_heads: 2               # GQA for efficiency  
  intermediate_size: 1024     # Larger FFN
  max_seq_len: 1024           # Max sequence length
  rope_theta: 10000.0
  rms_norm_eps: 1e-6
  tie_word_embeddings: true

# Scalability parameters - THE KEY INNOVATION!
scalable:
  # Training K range - model learns to work at ANY loop count
  train_k_min: 1              # Train with as few as 1 loop
  train_k_max: 16             # Train up to 16, should generalize beyond
  default_k: 8                # Default inference loops
  n_latent_iter: 1            # Reduced for memory
  
  # Convergence training - ensures quality scales with loops
  convergence_loss_weight: 0.1   # Penalize if later steps aren't better
  contrastive_loss_weight: 0.05  # Encourage higher confidence at later steps
  
  # Step embeddings - enables extrapolation beyond training K
  max_step_embeddings: 256    # Can extrapolate beyond this
  gate_init_bias: -2.0        # Conservative gate initialization

training:
  batch_size: 6                # Max that fits in 32GB GPU
  gradient_accumulation: 1     # DISABLED for speed!
  max_steps: 50000
  warmup_steps: 1000
  learning_rate: 5e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  
  # Precision
  use_bf16: true
  
  # Compilation - DISABLED (dynamic K breaks CUDA graphs)
  compile_enabled: false
  compile_mode: "reduce-overhead"

logging:
  log_interval: 10
  eval_interval: 10000  # Evaluate/benchmark every 10k steps
  save_interval: 2000
  output_dir: "./outputs"

tokenizer:
  name: "Qwen/Qwen2.5-0.5B"

data:
  # Using defaults from dataset.py:
  # - No limit on samples per dataset (uses all available)
  # - Default datasets: TeichAI/MiMo-V2-Flash-2300x, TeichAI/glm-4.7-2000x, 
  #   TeichAI/minimax-m2.1-1000x, TeichAI/claude-4.5-opus-high-reasoning-250x,
  #   open-thoughts/OpenThoughts3-1.2M, a-m-team/AM-DeepSeek-R1-0528-Distilled
