# =========================================================
# Pretraining Configuration for SmallLM (~800M params)
# =========================================================
# Train from scratch on FineWeb-Edu (100B tokens)
# Target: Chinchilla-optimal training (125 tokens/param)

# Model Architecture (~791M params)
model:
  vocab_size: 151936          # Qwen tokenizer
  dim: 1280                   # Model dimension
  n_layers: 28                # Transformer layers
  n_heads: 20                 # Attention heads
  n_kv_heads: 5               # KV heads (GQA 4:1)
  intermediate_size: 4480     # FFN intermediate size
  max_seq_len: 1024           # Sequence length
  rope_theta: 10000.0         # RoPE base
  rms_norm_eps: 1e-5          # RMSNorm epsilon
  tie_word_embeddings: true   # Share embeddings

# Dataset: FineWeb-Edu 100BT (100 billion tokens)
# Download size: ~270 GB (streamed, not all at once)
data:
  dataset_name: "HuggingFaceFW/fineweb-edu"
  dataset_config: "sample-100BT"   # 100B tokens
  tokenizer_name: "Qwen/Qwen2.5-0.5B"
  cache_dir: "./pretrain_cache"
  shuffle_buffer: 10000

# Training Settings
# Tokens per step = batch_size * grad_accum * seq_len
# = 4 * 16 * 1024 = 65,536 tokens/step
# Steps needed for 100B tokens = 100B / 65536 â‰ˆ 1,525,879
# We'll train for 500k steps (~33B tokens) as a start
training:
  batch_size: 4               # Per-GPU batch size
  gradient_accumulation: 16   # Effective batch = 64
  max_steps: 2441406          # Full 100B tokens pretraining
  warmup_steps: 2000          # LR warmup
  learning_rate: 3e-4         # Peak learning rate
  min_lr: 3e-5                # Minimum LR (10% of peak)
  weight_decay: 0.1           # AdamW weight decay
  max_grad_norm: 1.0          # Gradient clipping
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8

# Precision & Compilation
precision:
  dtype: "bfloat16"           # Training dtype
  compile_model: true         # Use torch.compile

# Logging & Checkpoints
logging:
  log_interval: 10            # Log every N steps
  save_interval: 5000         # Checkpoint every N steps
  output_dir: "./pretrain_outputs"

# =========================================================
# Training Plan:
# =========================================================
# Phase 1: Pretrain on FineWeb-Edu (THIS CONFIG)
#   - 500k steps = ~33B tokens
#   - Expected final loss: ~2.5-3.0
#   - Time estimate: ~3-5 days on RTX 5090
#
# Phase 2: Continue pretraining (increase max_steps)
#   - Resume from checkpoint
#   - Train to 1.5M steps = ~100B tokens
#
# Phase 3: Fine-tune on conversations
#   - Use original train.py with pretrained weights
#   - Much fewer steps needed (~10k-50k)
#   - This teaches the model conversation format
# =========================================================
