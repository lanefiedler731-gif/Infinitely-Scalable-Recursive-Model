# =========================================================
# SmallLM Training Cheatsheet
# =========================================================

# ---------------------------------------------------------
# TRAINING COMMANDS
# ---------------------------------------------------------

# RECOMMENDED: Full training from scratch (pretrain + instruction tune)
# ~6 hours for pretrain, ~1 hour for instruction tuning
python3 train.py --config config.yaml --pretrain

# Full training with larger dataset (~60 hours pretrain)
python3 train.py --config config.yaml --pretrain --large

# Just instruction tuning (no pretraining, uses random weights)
python3 train.py --config config.yaml

# Resume training from checkpoint
python3 train.py --config config.yaml --resume outputs/latest_checkpoint.pt

# Continue with model depth expansion
python3 train.py --config config.yaml --continue outputs/latest_checkpoint.pt

# ---------------------------------------------------------
# INFERENCE COMMANDS
# ---------------------------------------------------------

# Interactive chat
python3 inference.py --model outputs/best_model.pt --chat

# Single prompt
python3 inference.py --model outputs/best_model.pt --prompt "What is 2+2?"

# With custom temperature
python3 inference.py --model outputs/best_model.pt --temperature 0.7 --prompt "Tell me a story"

# Force CPU (for running while training on GPU)
python3 inference.py --model outputs/best_model.pt --cpu 8 --prompt "Hello"

# ---------------------------------------------------------
# EXPORT / CONVERSION
# ---------------------------------------------------------

# Export to HuggingFace format
python3 export_to_hf.py

# Convert to GGUF (for Ollama/llama.cpp)
python3 convert_to_gguf.py --model outputs/best_model.pt --output outputs/model.gguf --dtype f32

# Official GGUF conversion (after HF export)
.venv/bin/python convert_old.py hf_export/ --outfile outputs/model.gguf --outtype f32

# ---------------------------------------------------------
# MONITORING
# ---------------------------------------------------------

# Check GPU usage
nvidia-smi

# Watch GPU continuously
watch -n 1 nvidia-smi

# Check training progress from checkpoint
python3 -c "
import torch
from train import TrainingConfig
ckpt = torch.load('outputs/latest_checkpoint.pt', map_location='cpu', weights_only=False)
print(f'Step: {ckpt.get(\"global_step\")}')
print(f'Best Loss: {ckpt.get(\"best_val_loss\"):.4f}')
"

# ---------------------------------------------------------
# PRETRAINING DETAILS
# ---------------------------------------------------------

# sample-10BT:  10B tokens, ~27GB streaming, ~6 hours
# sample-100BT: 100B tokens, ~270GB streaming, ~60 hours

# For 800M model, Chinchilla optimal is 16B tokens
# 10B is 60% optimal (good enough for testing)
# 100B is 6x optimal (production quality)

# ---------------------------------------------------------
# QUICK REFERENCE
# ---------------------------------------------------------

# Model size: ~791M parameters
# Architecture: 28 layers, dim=1280, 20 heads, GQA 4:1
# Tokenizer: Qwen/Qwen2.5-0.5B (151k vocab)
# Training: BF16/FP8 with torch.compile

# Expected loss progression:
# - Random init: ~12.0 (ln(vocab_size))
# - After pretrain: ~2.5-3.0
# - After instruction tune: ~2.0-2.5